{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from helper import Gini, features_type\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#for NN model\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import *\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import History \n",
    "from time import time\n",
    "import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data #\n",
    "\n",
    "- Create train, test dataset\n",
    "- Create train target label\n",
    "- Create feature: miss, interval, ordinal, binary, nominal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_only = True\n",
    "save_cv = True\n",
    "\n",
    "\n",
    "#read data\n",
    "train = pd.read_csv(\"./input/train.csv\")\n",
    "train_label = train['target']\n",
    "train_id = train['id']\n",
    "del train['target'], train['id']\n",
    "\n",
    "test = pd.read_csv(\"./input/test.csv\")\n",
    "test_id = test['id']\n",
    "del test['id']\n",
    "\n",
    "#find missing value by each row and recode to column 'missing'\n",
    "train['missing'] = (train==-1).sum(axis=1).astype(float)\n",
    "test['missing'] = (test==-1).sum(axis=1).astype(float)\n",
    "\n",
    "# extract interval, ordinal, binary and nominal feature\n",
    "interval_fea, ordinal_fea, binary_fea, nominal_fea = features_type(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from matplotlib import pyplot\n",
    "FImodel = XGBClassifier()\n",
    "FImodel.fit(train, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "# from xgboost import plot_importance\n",
    "# plot_importance(FImodel, max_num_features = 15)\n",
    "# pyplot.show()\n",
    "\n",
    "#list the most important feature with descending\n",
    "sorted_idx = np.argsort(FImodel.feature_importances_)[::-1]\n",
    "# for index in sorted_idx:\n",
    "#     print([train.columns[index], FImodel.feature_importances_[index]])\n",
    "\n",
    "#extract first 15 most importacne features\n",
    "impo_fea=train.columns[sorted_idx[:15]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Nominal Data Encoding ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomianl  feature with one hot encoding\n",
    "for c in nominal_fea:\n",
    "    le = LabelEncoder()\n",
    "    x = le.fit_transform(pd.concat([train[nominal_fea], test[nominal_fea]])[c])\n",
    "    train[c] = le.transform(train[c])\n",
    "    test[c] = le.transform(test[c])\n",
    "    \n",
    "enc = OneHotEncoder()\n",
    "enc.fit(pd.concat([train[nominal_fea], test[nominal_fea]]))\n",
    "train_nominal = enc.transform(train[nominal_fea])\n",
    "test_nominal = enc.transform(test[nominal_fea])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Ordinal, binary, interval data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ordinal = train[ordinal_fea]\n",
    "test_ordinal = test[ordinal_fea]\n",
    "train_bin = train[binary_fea]\n",
    "test_bin =test[binary_fea]\n",
    "train_intv= train[interval_fea]\n",
    "test_intv=test[interval_fea]\n",
    "train_miss = train['missing'].values.reshape(-1,1)\n",
    "test_miss=test['missing'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 target_encoding ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoding_columns = list(interval_fea) + list(nominal_fea)\n",
    "for f in target_encoding_columns:\n",
    "    train[f + \"_tef\"], test[f + \"_tef\"] = target_encode(trn_series=train[f],\n",
    "                                         tst_series=test[f],\n",
    "                                         target=train_label,\n",
    "                                         min_samples_leaf=200,\n",
    "                                         smoothing=10,\n",
    "                                         noise_level=0)\n",
    "train_tef = train[[x for x in train.columns if 'tef' in x]]\n",
    "test_tef = test[[x for x in train.columns if 'tef' in x]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Combine all feature together, and get ready for training ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Building a training and testing list\n",
    "'''\n",
    "train_list = [train_nominal, train_ordinal, train_bin, train_intv, train_miss, train_tef] \n",
    "test_list = [test_nominal , test_ordinal, test_bin, test_intv, test_miss, test_tef]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "stack with sparse matrix and convert to array\n",
    "'''\n",
    "X = sparse.hstack(train_list).tocsr().toarray()\n",
    "tem_test = sparse.hstack(test_list).tocsr().toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=[]\n",
    "train_impo= train[impo_fea]\n",
    "test_impo= test[impo_fea]\n",
    "for i in range(train_impo.shape[1]):\n",
    "    X_test.append(test_impo.values[:,i].reshape(-1,1))\n",
    "X_test.append(tem_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training NN Model with Keras # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Build the model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model():\n",
    "    inputs = []\n",
    "    layers = []\n",
    "    for _ in impo_fea:\n",
    "        input_impo = Input(shape=(1, ), dtype='float32')\n",
    "        net_impo = Dense(64, kernel_initializer='he_normal')(input_impo)\n",
    "        net_impo = PReLU()(net_impo)\n",
    "        net_impo = BatchNormalization()(net_impo)\n",
    "        net_impo = Dropout(0.25)(net_impo)\n",
    "        inputs.append(input_impo)\n",
    "        layers.append(net_impo)\n",
    "\n",
    "    input_num = Input(shape=(X.shape[1],), dtype='float32')\n",
    "    layers.append(input_num)\n",
    "    inputs.append(input_num)\n",
    "\n",
    "    flatten = Concatenate(axis=-1)(layers)\n",
    "\n",
    "    fc1 = Dense(512, kernel_initializer='he_normal')(flatten)\n",
    "    fc1 = PReLU()(fc1)\n",
    "    fc1 = BatchNormalization()(fc1)\n",
    "    fc1 = Dropout(0.5)(fc1)\n",
    "    fc1 = Dense(64, kernel_initializer='he_normal')(fc1)\n",
    "    fc1 = PReLU()(fc1)\n",
    "    fc1 = BatchNormalization()(fc1)\n",
    "    fc1 = Dropout(0.25)(fc1)\n",
    "\n",
    "    outputs = Dense(1, kernel_initializer='he_normal', activation='sigmoid')(fc1)\n",
    "    model = Model(inputs = inputs, outputs = outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Start to Train ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 297606 samples, validate on 297606 samples\n",
      "Epoch 1/2\n",
      " - 65s - loss: 0.2690 - val_loss: 0.1548\n",
      "Epoch 2/2\n",
      " - 60s - loss: 0.1612 - val_loss: 0.1540\n",
      "Train on 297606 samples, validate on 297606 samples\n",
      "Epoch 1/2\n",
      " - 72s - loss: 0.2695 - val_loss: 0.1602\n",
      "Epoch 2/2\n",
      " - 66s - loss: 0.1639 - val_loss: 0.1547\n",
      "0.226818691295\n",
      "Total training time:  0:06:35.754765\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NFOLDS = 5\n",
    "\"\"\"\n",
    "cv_train = np.zeros(len(train_label))\n",
    "cv_pred = np.zeros(len(test_id))\n",
    "\n",
    "#validation fold\n",
    "NFOLDS = 2\n",
    "kfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=218)\n",
    "\n",
    "begintime = time()\n",
    "history = History()\n",
    "loss_record= defaultdict(list)\n",
    "Gini_record={}\n",
    "Gini_record['Gini']=[]\n",
    "n=1\n",
    "for (train_index, valid_index) in kfold.split(X, train_label):\n",
    "    x_train = X[train_index]\n",
    "    y_train = train_label[train_index]\n",
    "    x_valid = X[valid_index]\n",
    "    y_valid = train_label[valid_index]\n",
    "\n",
    "    x_train_impo = train_impo.values[train_index]\n",
    "    x_valid_impo = train_impo.values[valid_index]\n",
    "\n",
    "\n",
    "   # get x_train x_test cat\n",
    "    train_list, valid_list = [], []\n",
    "    for i in range(x_train_impo.shape[1]):\n",
    "        train_list.append(x_train_impo[:, i].reshape(-1, 1))\n",
    "        valid_list.append(x_valid_impo[:, i].reshape(-1, 1))\n",
    "\n",
    "    train_list.append(x_train)\n",
    "    valid_list.append(x_valid)\n",
    "    model = nn_model()\n",
    "\n",
    "    hist = model.fit(train_list, y_train, epochs=2, batch_size=512, verbose=2, validation_data=[valid_list, y_valid], callbacks=[history])\n",
    "    cv_train[valid_index] += model.predict(x=valid_list, batch_size=512, verbose=0)[:,0]\n",
    "    Gini_scores= Gini(train_label[valid_index], cv_train[valid_index])\n",
    "#     print('Current Folder Scores: ',Gini_scores)\n",
    "    cv_pred += model.predict(x=X_test, batch_size=512, verbose=0)[:,0]\n",
    "    \n",
    "    \n",
    "    #loss record\n",
    "    loss_name='loss_'+ str(n)\n",
    "    val_loss_name='val_loss_'+str(n)\n",
    "    loss_record[loss_name]=hist.history['loss']\n",
    "    loss_record[val_loss_name]=hist.history['val_loss']\n",
    "    n+=1\n",
    "    \n",
    "    #Gini Record\n",
    "    Gini_record['Gini'].append(Gini_scores)\n",
    "    \n",
    "    \n",
    "\n",
    "print(Gini(train_label, cv_train ))\n",
    "print(\"Total training time: \",str(datetime.timedelta(seconds=time() - begintime)))\n",
    "\n",
    "# pd.DataFrame({'id': test_id, 'target': cv_pred * 1./ (NFOLDS)}).to_csv('../model/nnModelpred.csv', index=False)\n",
    "pd.DataFrame(loss_record).to_csv('../model/loss_Record.csv', index=False)\n",
    "pd.DataFrame(Gini_Record).to_csv('../model/Gini_Record.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction= model.predict(x=X_test, batch_size=512, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(loss_record).to_csv('./model/loss_Record.csv', index=False)\n",
    "pd.DataFrame(Gini_record).to_csv('./model/Gini_Record.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'loss'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(892816, 1)\n"
     ]
    }
   ],
   "source": [
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-89c1f56fdb51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "for layer in nn_model.layers:\n",
    "    print(layer.get_output_at(0).get_shape().as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
