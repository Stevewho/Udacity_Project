{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Train a Quadcopter How to Fly\n",
    "\n",
    "Design an agent to fly a quadcopter, and then train it using a reinforcement learning algorithm of your choice! \n",
    "\n",
    "Try to apply the techniques you have learnt, but also feel free to come up with innovative ideas and test them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Take a look at the files in the directory to better understand the structure of the project. \n",
    "\n",
    "- `task.py`: Define your task (environment) in this file.\n",
    "- `agents/`: Folder containing reinforcement learning agents.\n",
    "    - `policy_search.py`: A sample agent has been provided here.\n",
    "    - `agent.py`: Develop your agent here.\n",
    "- `physics_sim.py`: This file contains the simulator for the quadcopter.  **DO NOT MODIFY THIS FILE**.\n",
    "\n",
    "For this project, you will define your own task in `task.py`.  Although we have provided a example task to get you started, you are encouraged to change it.  Later in this notebook, you will learn more about how to amend this file.\n",
    "\n",
    "You will also design a reinforcement learning agent in `agent.py` to complete your chosen task.  \n",
    "\n",
    "You are welcome to create any additional files to help you to organize your code.  For instance, you may find it useful to define a `model.py` file defining any needed neural network architectures.\n",
    "\n",
    "## Controlling the Quadcopter\n",
    "\n",
    "We provide a sample agent in the code cell below to show you how to use the sim to control the quadcopter.  This agent is even simpler than the sample agent that you'll examine (in `agents/policy_search.py`) later in this notebook!\n",
    "\n",
    "The agent controls the quadcopter by setting the revolutions per second on each of its four rotors.  The provided agent in the `Basic_Agent` class below always selects a random action for each of the four rotors.  These four speeds are returned by the `act` method as a list of four floating-point numbers.  \n",
    "\n",
    "For this project, the agent that you will implement in `agents/agent.py` will have a far more intelligent method for selecting actions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Basic_Agent():\n",
    "    def __init__(self, task):\n",
    "        self.task = task\n",
    "    \n",
    "    def act(self):\n",
    "        new_thrust = random.gauss(450., 25.)\n",
    "        return [new_thrust + random.gauss(0., 1.) for x in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to have the agent select actions to control the quadcopter.  \n",
    "\n",
    "Feel free to change the provided values of `runtime`, `init_pose`, `init_velocities`, and `init_angle_velocities` below to change the starting conditions of the quadcopter.\n",
    "\n",
    "The `labels` list below annotates statistics that are saved while running the simulation.  All of this information is saved in a text file `data.txt` and stored in the dictionary `results`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from task import Task\n",
    "\n",
    "# Modify the values below to give the quadcopter a different starting position.\n",
    "runtime = 5.                                     # time limit of the episode\n",
    "init_pose = np.array([0., 0., 10., 0., 0., 0.])  # initial pose\n",
    "init_velocities = np.array([0.1, 0.5, 0.6])         # initial velocities\n",
    "init_angle_velocities = np.array([0., 0., 0.])   # initial angle velocities\n",
    "file_output = 'data.txt'                         # file name for saved results\n",
    "\n",
    "# Setup\n",
    "task = Task(init_pose, init_velocities, init_angle_velocities, runtime)\n",
    "agent = Basic_Agent(task)\n",
    "done = False\n",
    "labels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',\n",
    "          'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',\n",
    "          'psi_velocity', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4']\n",
    "results = {x : [] for x in labels}\n",
    "\n",
    "# Run the simulation, and save the results.\n",
    "with open(file_output, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(labels)\n",
    "    while True:\n",
    "        rotor_speeds = agent.act()\n",
    "        _, _, done = task.step(rotor_speeds)\n",
    "        to_write = [task.sim.time] + list(task.sim.pose) + list(task.sim.v) + list(task.sim.angular_v) + list(rotor_speeds)\n",
    "        for ii in range(len(labels)):\n",
    "            results[labels[ii]].append(to_write[ii])\n",
    "            print(labels[ii],to_write[ii],'\\n')\n",
    "        writer.writerow(to_write)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to visualize how the position of the quadcopter evolved during the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(results['time'], results['x'], label='x')\n",
    "plt.plot(results['time'], results['y'], label='y')\n",
    "plt.plot(results['time'], results['z'], label='z')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell visualizes the velocity of the quadcopter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results['time'], results['x_velocity'], label='x_hat')\n",
    "plt.plot(results['time'], results['y_velocity'], label='y_hat')\n",
    "plt.plot(results['time'], results['z_velocity'], label='z_hat')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can plot the Euler angles (the rotation of the quadcopter over the $x$-, $y$-, and $z$-axes),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results['time'], results['phi'], label='phi')\n",
    "plt.plot(results['time'], results['theta'], label='theta')\n",
    "plt.plot(results['time'], results['psi'], label='psi')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before plotting the velocities (in radians per second) corresponding to each of the Euler angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results['time'], results['phi_velocity'], label='phi_velocity')\n",
    "plt.plot(results['time'], results['theta_velocity'], label='theta_velocity')\n",
    "plt.plot(results['time'], results['psi_velocity'], label='psi_velocity')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use the code cell below to print the agent's choice of actions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results['time'], results['rotor_speed1'], label='Rotor 1 revolutions / second')\n",
    "plt.plot(results['time'], results['rotor_speed2'], label='Rotor 2 revolutions / second')\n",
    "plt.plot(results['time'], results['rotor_speed3'], label='Rotor 3 revolutions / second')\n",
    "plt.plot(results['time'], results['rotor_speed4'], label='Rotor 4 revolutions / second')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When specifying a task, you will derive the environment state from the simulator.  Run the code cell below to print the values of the following variables at the end of the simulation:\n",
    "- `task.sim.pose` (the position of the quadcopter in ($x,y,z$) dimensions and the Euler angles),\n",
    "- `task.sim.v` (the velocity of the quadcopter in ($x,y,z$) dimensions), and\n",
    "- `task.sim.angular_v` (radians/second for each of the three Euler angles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pose, velocity, and angular velocity of the quadcopter at the end of the episode\n",
    "print(task.sim.pose)\n",
    "print(task.sim.v)\n",
    "print(task.sim.angular_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sample task in `task.py`, we use the 6-dimensional pose of the quadcopter to construct the state of the environment at each timestep.  However, when amending the task for your purposes, you are welcome to expand the size of the state vector by including the velocity information.  You can use any combination of the pose, velocity, and angular velocity - feel free to tinker here, and construct the state to suit your task.\n",
    "\n",
    "## The Task\n",
    "\n",
    "A sample task has been provided for you in `task.py`.  Open this file in a new window now. \n",
    "\n",
    "The `__init__()` method is used to initialize several variables that are needed to specify the task.  \n",
    "- The simulator is initialized as an instance of the `PhysicsSim` class (from `physics_sim.py`).  \n",
    "- Inspired by the methodology in the original DDPG paper, we make use of action repeats.  For each timestep of the agent, we step the simulation `action_repeats` timesteps.  If you are not familiar with action repeats, please read the **Results** section in [the DDPG paper](https://arxiv.org/abs/1509.02971).\n",
    "- We set the number of elements in the state vector.  For the sample task, we only work with the 6-dimensional pose information.  To set the size of the state (`state_size`), we must take action repeats into account.  \n",
    "- The environment will always have a 4-dimensional action space, with one entry for each rotor (`action_size=4`). You can set the minimum (`action_low`) and maximum (`action_high`) values of each entry here.\n",
    "- The sample task in this provided file is for the agent to reach a target position.  We specify that target position as a variable.\n",
    "\n",
    "The `reset()` method resets the simulator.  The agent should call this method every time the episode ends.  You can see an example of this in the code cell below.\n",
    "\n",
    "The `step()` method is perhaps the most important.  It accepts the agent's choice of action `rotor_speeds`, which is used to prepare the next state to pass on to the agent.  Then, the reward is computed from `get_reward()`.  The episode is considered done if the time limit has been exceeded, or the quadcopter has travelled outside of the bounds of the simulation.\n",
    "\n",
    "In the next section, you will learn how to test the performance of an agent on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "The sample agent given in `agents/policy_search.py` uses a very simplistic linear policy to directly compute the action vector as a dot product of the state vector and a matrix of weights. Then, it randomly perturbs the parameters by adding some Gaussian noise, to produce a different policy. Based on the average reward obtained in each episode (`score`), it keeps track of the best set of parameters found so far, how the score is changing, and accordingly tweaks a scaling factor to widen or tighten the noise.\n",
    "\n",
    "Run the code cell below to see how the agent performs on the sample task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from agents.policy_search import PolicySearch_Agent\n",
    "from task import Task\n",
    "\n",
    "num_episodes = 1000\n",
    "target_pos = np.array([0., 0., 10.])\n",
    "task = Task(target_pos=target_pos)\n",
    "agent = PolicySearch_Agent(task) \n",
    "\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    state = agent.reset_episode() # start a new episode\n",
    "    \n",
    "    while True:\n",
    "        action = agent.act(state) \n",
    "#         print(state)\n",
    "        next_state, reward, done = task.step(action)\n",
    "#         print('reward',reward, 'done', done)\n",
    "        agent.step(reward, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"\\rEpisode = {:4d}, score = {:7.3f} (best = {:7.3f}), noise_scale = {}\".format(\n",
    "                i_episode, agent.score, agent.best_score, agent.noise_scale), end=\"\")  # [debug]\n",
    "            break\n",
    "            \n",
    "        \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agent should perform very poorly on this task.  And that's where you come in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Define the Task, Design the Agent, and Train Your Agent!\n",
    "\n",
    "Amend `task.py` to specify a task of your choosing.  If you're unsure what kind of task to specify, you may like to teach your quadcopter to takeoff, hover in place, land softly, or reach a target pose.  \n",
    "\n",
    "After specifying your task, use the sample agent in `agents/policy_search.py` as a template to define your own agent in `agents/agent.py`.  You can borrow whatever you need from the sample agent, including ideas on how you might modularize your code (using helper methods like `act()`, `learn()`, `reset_episode()`, etc.).\n",
    "\n",
    "Note that it is **highly unlikely** that the first agent and task that you specify will learn well.  You will likely have to tweak various hyperparameters and the reward function for your task until you arrive at reasonably good behavior.\n",
    "\n",
    "As you develop your agent, it's important to keep an eye on how it's performing. Use the code above as inspiration to build in a mechanism to log/save the total rewards obtained in each episode to file.  If the episode rewards are gradually increasing, this is an indication that your agent is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenhu/Documents/Udacity/ML course/Udacity_Project/Quadcopter /Version2/agents/nnagent.py:140: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  i = merge([inp,act],mode='concat')\n",
      "/Users/stevenhu/anaconda3/envs/run/lib/python3.6/site-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/Users/stevenhu/anaconda3/envs/run/lib/python3.6/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_98 (Dense)                (None, 8)            152         input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_99 (Dense)                (None, 32)           288         dense_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_97 (Dense)                (None, 32)           608         input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 32)           0           dense_99[0][0]                   \n",
      "                                                                 dense_97[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_101 (Dense)               (None, 8)            264         add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_102 (Dense)               (None, 32)           288         dense_101[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_100 (Dense)               (None, 32)           1056        add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 32)           0           dense_102[0][0]                  \n",
      "                                                                 dense_100[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_104 (Dense)               (None, 16)           528         add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_105 (Dense)               (None, 64)           1088        dense_104[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_103 (Dense)               (None, 64)           2112        add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 64)           0           dense_105[0][0]                  \n",
      "                                                                 dense_103[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_107 (Dense)               (None, 4)            260         add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_108 (Dense)               (None, 4)            20          dense_107[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_106 (Dense)               (None, 4)            260         add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 4)            0           dense_108[0][0]                  \n",
      "                                                                 dense_106[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 4)            0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 4)            0           activation_5[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 6,924\n",
      "Trainable params: 6,924\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_5 (Merge)                 (None, 22)           0           input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_110 (Dense)               (None, 16)           368         merge_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_111 (Dense)               (None, 64)           1088        dense_110[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_109 (Dense)               (None, 64)           1472        merge_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 64)           0           dense_111[0][0]                  \n",
      "                                                                 dense_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_113 (Dense)               (None, 8)            520         add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_114 (Dense)               (None, 32)           288         dense_113[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_112 (Dense)               (None, 32)           2080        add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 32)           0           dense_114[0][0]                  \n",
      "                                                                 dense_112[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_116 (Dense)               (None, 8)            264         add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_117 (Dense)               (None, 32)           288         dense_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_115 (Dense)               (None, 32)           1056        add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 32)           0           dense_117[0][0]                  \n",
      "                                                                 dense_115[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_119 (Dense)               (None, 4)            132         add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_120 (Dense)               (None, 1)            5           dense_119[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_118 (Dense)               (None, 1)            33          add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 1)            0           dense_120[0][0]                  \n",
      "                                                                 dense_118[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,188\n",
      "Trainable params: 7,594\n",
      "Non-trainable params: 7,594\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0 / 100 noise_level 1.0\n",
      "episode done in 24 steps, total reward 69.0\n",
      "ep 1 / 100 noise_level 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenhu/Documents/Udacity/ML course/Udacity_Project/Quadcopter /Version2/task.py:37: RuntimeWarning: overflow encountered in exp\n",
      "  reward = np.exp(-(np.abs(target_pos  - current_pos)/(0.1*target_pos))).sum()+ np.exp(-(current_pos[2]-target_pos[2])/0.1*target_pos[2])\n",
      "/Users/stevenhu/Documents/Udacity/ML course/Udacity_Project/Quadcopter /Version2/physics_sim.py:114: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  J = V / n * D\n",
      "/Users/stevenhu/Documents/Udacity/ML course/Udacity_Project/Quadcopter /Version2/physics_sim.py:114: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  J = V / n * D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode done in 26 steps, total reward 75.0\n",
      "ep 2 / 100 noise_level 0.98\n",
      "episode done in 26 steps, total reward 75.0\n",
      "ep 3 / 100 noise_level 0.97\n",
      "Epoch 1/1\n",
      " - 0s - loss: 12250.1113\n",
      "Epoch 1/1\n",
      " - 0s - loss: -1.2496e+02\n",
      "episode done in 23 steps, total reward 66.0\n",
      "ep 4 / 100 noise_level 0.96\n",
      "Epoch 1/1\n",
      " - 0s - loss: 14446.3965\n",
      "Epoch 1/1\n",
      " - 0s - loss: -5.9763e+01\n",
      "episode done in 23 steps, total reward 68.0\n",
      "ep 5 / 100 noise_level 0.95\n",
      "Epoch 1/1\n",
      " - 0s - loss: 10666.1660\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.8464\n",
      "episode done in 62 steps, total reward 183.0\n",
      "ep 6 / 100 noise_level 0.94\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8221.6152\n",
      "Epoch 1/1\n",
      " - 0s - loss: 35.6927\n",
      "episode done in 25 steps, total reward 72.0\n",
      "ep 7 / 100 noise_level 0.93\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9427.2129\n",
      "Epoch 1/1\n",
      " - 0s - loss: 61.5635\n",
      "episode done in 27 steps, total reward 78.0\n",
      "ep 8 / 100 noise_level 0.92\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7629.4263\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8055\n",
      "episode done in 23 steps, total reward 66.0\n",
      "ep 9 / 100 noise_level 0.91\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5900.7056\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.5428\n",
      "episode done in 23 steps, total reward 66.0\n",
      "ep 10 / 100 noise_level 0.9\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6146.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.3960\n",
      "episode done in 24 steps, total reward 70.0\n",
      "ep 11 / 100 noise_level 0.89\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5599.9121\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16.2418\n",
      "episode done in 24 steps, total reward 69.0\n",
      "ep 12 / 100 noise_level 0.88\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6496.9087\n",
      "Epoch 1/1\n",
      " - 0s - loss: 19.0419\n",
      "episode done in 24 steps, total reward 69.0\n",
      "ep 13 / 100 noise_level 0.87\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5191.9526\n",
      "Epoch 1/1\n",
      " - 0s - loss: 27.4691\n",
      "episode done in 24 steps, total reward 70.0\n",
      "ep 14 / 100 noise_level 0.86\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2956.6660\n",
      "Epoch 1/1\n",
      " - 0s - loss: 37.5932\n",
      "episode done in 25 steps, total reward 72.0\n",
      "ep 15 / 100 noise_level 0.85\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3846.7490\n",
      "Epoch 1/1\n",
      " - 0s - loss: -4.5298e+01\n",
      "episode done in 26 steps, total reward 77.0\n",
      "ep 16 / 100 noise_level 0.84\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2508.2183\n",
      "Epoch 1/1\n",
      " - 0s - loss: 69.6034\n",
      "episode done in 22 steps, total reward 63.0\n",
      "ep 17 / 100 noise_level 0.83\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2750.3887\n",
      "Epoch 1/1\n",
      " - 0s - loss: 70.8940\n",
      "episode done in 23 steps, total reward 66.0\n",
      "ep 18 / 100 noise_level 0.82\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3105.2275\n",
      "Epoch 1/1\n",
      " - 0s - loss: 34.5264\n",
      "episode done in 84 steps, total reward 105.519292547\n",
      "ep 19 / 100 noise_level 0.81\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2341.3120\n",
      "Epoch 1/1\n",
      " - 0s - loss: 21.5578\n",
      "episode done in 25 steps, total reward 72.0\n",
      "ep 20 / 100 noise_level 0.8\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3253.0574\n",
      "Epoch 1/1\n",
      " - 0s - loss: 28.1536\n",
      "episode done in 24 steps, total reward 71.0\n",
      "ep 21 / 100 noise_level 0.79\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1612.1173\n",
      "Epoch 1/1\n",
      " - 0s - loss: 11.7638\n",
      "episode done in 25 steps, total reward 72.0\n",
      "ep 22 / 100 noise_level 0.78\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1371.3739\n",
      "Epoch 1/1\n",
      " - 0s - loss: 44.7585\n",
      "episode done in 25 steps, total reward 72.0\n",
      "ep 23 / 100 noise_level 0.77\n",
      "Epoch 1/1\n",
      " - 0s - loss: 814.4598\n",
      "Epoch 1/1\n",
      " - 0s - loss: 26.2159\n",
      "episode done in 23 steps, total reward 68.0\n",
      "ep 24 / 100 noise_level 0.76\n",
      "Epoch 1/1\n",
      " - 0s - loss: 925.0242\n",
      "Epoch 1/1\n",
      " - 0s - loss: 22.1189\n",
      "episode done in 26 steps, total reward 76.0\n",
      "ep 25 / 100 noise_level 0.75\n",
      "Epoch 1/1\n",
      " - 0s - loss: 705.2130\n",
      "Epoch 1/1\n",
      " - 0s - loss: 33.3023\n",
      "episode done in 24 steps, total reward 70.0\n",
      "ep 26 / 100 noise_level 0.74\n",
      "Epoch 1/1\n",
      " - 0s - loss: 721.3290\n",
      "Epoch 1/1\n",
      " - 0s - loss: 22.0011\n",
      "episode done in 23 steps, total reward 68.0\n",
      "ep 27 / 100 noise_level 0.73\n",
      "Epoch 1/1\n",
      " - 0s - loss: 422.0228\n",
      "Epoch 1/1\n",
      " - 0s - loss: 33.4087\n",
      "episode done in 25 steps, total reward 72.0\n",
      "ep 28 / 100 noise_level 0.72\n",
      "Epoch 1/1\n",
      " - 0s - loss: 412.8015\n",
      "Epoch 1/1\n",
      " - 0s - loss: 33.3922\n",
      "episode done in 25 steps, total reward 72.0\n",
      "ep 29 / 100 noise_level 0.71\n",
      "Epoch 1/1\n",
      " - 0s - loss: 361.3625\n",
      "Epoch 1/1\n",
      " - 0s - loss: 28.0910\n",
      "episode done in 25 steps, total reward 72.0\n",
      "ep 30 / 100 noise_level 0.7\n",
      "Epoch 1/1\n",
      " - 0s - loss: 251.9651\n",
      "Epoch 1/1\n",
      " - 0s - loss: 34.1662\n",
      "episode done in 26 steps, total reward 75.0\n",
      "ep 31 / 100 noise_level 0.69\n",
      "Epoch 1/1\n",
      " - 0s - loss: 324.0830\n",
      "Epoch 1/1\n",
      " - 0s - loss: 36.7501\n",
      "episode done in 27 steps, total reward 78.0\n",
      "ep 32 / 100 noise_level 0.68\n",
      "Epoch 1/1\n",
      " - 0s - loss: 344.7015\n",
      "Epoch 1/1\n",
      " - 0s - loss: 43.4486\n",
      "episode done in 25 steps, total reward 72.0\n",
      "ep 33 / 100 noise_level 0.67\n",
      "Epoch 1/1\n",
      " - 0s - loss: 478.3916\n",
      "Epoch 1/1\n",
      " - 0s - loss: 28.6125\n",
      "episode done in 23 steps, total reward 66.0\n",
      "ep 34 / 100 noise_level 0.66\n",
      "Epoch 1/1\n",
      " - 0s - loss: 172.1758\n",
      "Epoch 1/1\n",
      " - 0s - loss: 44.5449\n",
      "episode done in 24 steps, total reward 69.0\n",
      "ep 35 / 100 noise_level 0.65\n",
      "Epoch 1/1\n",
      " - 0s - loss: 171.3864\n",
      "Epoch 1/1\n",
      " - 0s - loss: 31.6623\n",
      "episode done in 24 steps, total reward 69.0\n",
      "ep 36 / 100 noise_level 0.64\n",
      "Epoch 1/1\n",
      " - 0s - loss: 179.4467\n",
      "Epoch 1/1\n",
      " - 0s - loss: 30.8841\n",
      "episode done in 24 steps, total reward 71.0\n",
      "ep 37 / 100 noise_level 0.63\n",
      "Epoch 1/1\n",
      " - 0s - loss: 187.4312\n",
      "Epoch 1/1\n",
      " - 0s - loss: 24.5121\n",
      "episode done in 24 steps, total reward 70.0\n",
      "ep 38 / 100 noise_level 0.62\n",
      "Epoch 1/1\n",
      " - 0s - loss: 159.7916\n",
      "Epoch 1/1\n",
      " - 0s - loss: 30.4415\n",
      "episode done in 25 steps, total reward 74.0\n",
      "ep 39 / 100 noise_level 0.61\n",
      "Epoch 1/1\n",
      " - 0s - loss: 93.0559\n",
      "Epoch 1/1\n",
      " - 0s - loss: 29.9926\n",
      "episode done in 23 steps, total reward 67.0\n",
      "ep 40 / 100 noise_level 0.6\n",
      "Epoch 1/1\n",
      " - 0s - loss: 91.9483\n",
      "Epoch 1/1\n",
      " - 0s - loss: 26.2127\n",
      "episode done in 24 steps, total reward 70.0\n",
      "ep 41 / 100 noise_level 0.59\n",
      "Epoch 1/1\n",
      " - 0s - loss: 125.1647\n",
      "Epoch 1/1\n",
      " - 0s - loss: 21.4365\n",
      "episode done in 25 steps, total reward 74.0\n",
      "ep 42 / 100 noise_level 0.58\n",
      "Epoch 1/1\n",
      " - 0s - loss: 318.0619\n",
      "Epoch 1/1\n",
      " - 0s - loss: 19.5849\n",
      "episode done in 23 steps, total reward 68.0\n",
      "ep 43 / 100 noise_level 0.57\n",
      "Epoch 1/1\n",
      " - 0s - loss: 119.0663\n",
      "Epoch 1/1\n",
      " - 0s - loss: 21.8527\n",
      "episode done in 23 steps, total reward 67.0\n",
      "ep 44 / 100 noise_level 0.56\n",
      "Epoch 1/1\n",
      " - 0s - loss: 445.5674\n",
      "Epoch 1/1\n",
      " - 0s - loss: 18.6495\n",
      "episode done in 25 steps, total reward 72.0\n",
      "ep 45 / 100 noise_level 0.55\n",
      "Epoch 1/1\n",
      " - 0s - loss: 66.5789\n",
      "Epoch 1/1\n",
      " - 0s - loss: 28.4181\n",
      "episode done in 22 steps, total reward 65.0\n",
      "ep 46 / 100 noise_level 0.54\n",
      "Epoch 1/1\n",
      " - 0s - loss: 64.3067\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16.5507\n",
      "episode done in 24 steps, total reward 70.0\n",
      "ep 47 / 100 noise_level 0.53\n",
      "Epoch 1/1\n",
      " - 0s - loss: 381.6675\n",
      "Epoch 1/1\n",
      " - 0s - loss: 18.4556\n",
      "episode done in 24 steps, total reward 69.0\n",
      "ep 48 / 100 noise_level 0.52\n",
      "Epoch 1/1\n",
      " - 0s - loss: 75.7585\n",
      "Epoch 1/1\n",
      " - 0s - loss: 28.5172\n",
      "episode done in 23 steps, total reward 68.0\n",
      "ep 49 / 100 noise_level 0.51\n",
      "Epoch 1/1\n",
      " - 0s - loss: 64.3196\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16.0213\n",
      "episode done in 26 steps, total reward 76.0\n",
      "ep 50 / 100 noise_level 0.5\n",
      "Epoch 1/1\n",
      " - 0s - loss: 48.5843\n",
      "Epoch 1/1\n",
      " - 0s - loss: 21.1286\n",
      "episode done in 25 steps, total reward 72.0\n",
      "ep 51 / 100 noise_level 0.49\n",
      "Epoch 1/1\n",
      " - 0s - loss: 57.9094\n",
      "Epoch 1/1\n",
      " - 0s - loss: 10.7092\n",
      "episode done in 25 steps, total reward 72.0\n",
      "ep 52 / 100 noise_level 0.48\n",
      "Epoch 1/1\n",
      " - 0s - loss: 81.7765\n",
      "Epoch 1/1\n",
      " - 0s - loss: 11.4528\n",
      "episode done in 25 steps, total reward 74.0\n",
      "ep 53 / 100 noise_level 0.47\n",
      "Epoch 1/1\n",
      " - 0s - loss: 45.2491\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16.2224\n",
      "episode done in 25 steps, total reward 73.0\n",
      "ep 54 / 100 noise_level 0.46\n",
      "Epoch 1/1\n",
      " - 0s - loss: 48.7004\n",
      "Epoch 1/1\n",
      " - 0s - loss: 25.9940\n",
      "episode done in 25 steps, total reward 73.0\n",
      "ep 55 / 100 noise_level 0.45\n",
      "Epoch 1/1\n",
      " - 0s - loss: 41.1719\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16.6446\n",
      "episode done in 24 steps, total reward 70.0\n",
      "ep 56 / 100 noise_level 0.44\n",
      "Epoch 1/1\n",
      " - 0s - loss: 129.6454\n",
      "Epoch 1/1\n",
      " - 0s - loss: 27.7931\n",
      "episode done in 28 steps, total reward 82.0\n",
      "ep 57 / 100 noise_level 0.43\n",
      "Epoch 1/1\n",
      " - 0s - loss: 288.1204\n",
      "Epoch 1/1\n",
      " - 0s - loss: 22.8148\n",
      "episode done in 84 steps, total reward 93.5389817968\n",
      "ep 58 / 100 noise_level 0.42\n",
      "Epoch 1/1\n",
      " - 0s - loss: 20.5184\n",
      "Epoch 1/1\n",
      " - 0s - loss: 13.4415\n",
      "episode done in 43 steps, total reward 128.0\n",
      "ep 59 / 100 noise_level 0.41\n",
      "Epoch 1/1\n",
      " - 0s - loss: 29.6406\n",
      "Epoch 1/1\n",
      " - 0s - loss: 15.6758\n",
      "episode done in 42 steps, total reward 123.0\n",
      "ep 60 / 100 noise_level 0.4\n",
      "Epoch 1/1\n",
      " - 0s - loss: 27.2965\n",
      "Epoch 1/1\n",
      " - 0s - loss: 10.1576\n",
      "episode done in 29 steps, total reward 86.0\n",
      "ep 61 / 100 noise_level 0.39\n",
      "Epoch 1/1\n",
      " - 0s - loss: 84.9285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 0s - loss: 7.6496\n",
      "episode done in 26 steps, total reward 77.0\n",
      "ep 62 / 100 noise_level 0.38\n",
      "Epoch 1/1\n",
      " - 0s - loss: 54.1691\n",
      "Epoch 1/1\n",
      " - 0s - loss: 10.3825\n",
      "episode done in 26 steps, total reward 75.0\n",
      "ep 63 / 100 noise_level 0.37\n",
      "Epoch 1/1\n",
      " - 0s - loss: 29.3748\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.8756\n",
      "episode done in 27 steps, total reward 78.0\n",
      "ep 64 / 100 noise_level 0.36\n",
      "Epoch 1/1\n",
      " - 0s - loss: 25.6260\n",
      "Epoch 1/1\n",
      " - 0s - loss: 13.8274\n",
      "episode done in 27 steps, total reward 79.0\n",
      "ep 65 / 100 noise_level 0.35\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16.2921\n",
      "Epoch 1/1\n",
      " - 0s - loss: 12.9018\n",
      "episode done in 26 steps, total reward 76.0\n",
      "ep 66 / 100 noise_level 0.34\n",
      "Epoch 1/1\n",
      " - 0s - loss: 46.0270\n",
      "Epoch 1/1\n",
      " - 0s - loss: 11.9092\n",
      "episode done in 27 steps, total reward 78.0\n",
      "ep 67 / 100 noise_level 0.33\n",
      "Epoch 1/1\n",
      " - 0s - loss: 20.9609\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.4209\n",
      "episode done in 27 steps, total reward 80.0\n",
      "ep 68 / 100 noise_level 0.32\n",
      "Epoch 1/1\n",
      " - 0s - loss: 17.1611\n",
      "Epoch 1/1\n",
      " - 0s - loss: 13.0487\n",
      "episode done in 56 steps, total reward 167.0\n",
      "ep 69 / 100 noise_level 0.31\n",
      "Epoch 1/1\n",
      " - 0s - loss: 40.7877\n",
      "Epoch 1/1\n",
      " - 0s - loss: 24.3256\n",
      "episode done in 84 steps, total reward 73.5925861823\n",
      "ep 70 / 100 noise_level 0.3\n",
      "Epoch 1/1\n",
      " - 0s - loss: 13.3128\n",
      "Epoch 1/1\n",
      " - 0s - loss: 22.9982\n",
      "episode done in 84 steps, total reward 64.2063892166\n",
      "ep 71 / 100 noise_level 0.29\n",
      "Epoch 1/1\n",
      " - 0s - loss: 59.8597\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.9035\n",
      "episode done in 84 steps, total reward 64.3917173893\n",
      "ep 72 / 100 noise_level 0.28\n",
      "Epoch 1/1\n",
      " - 0s - loss: 640.5093\n",
      "Epoch 1/1\n",
      " - 0s - loss: 14.6594\n",
      "episode done in 27 steps, total reward 79.0\n",
      "ep 73 / 100 noise_level 0.27\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.8703\n",
      "Epoch 1/1\n",
      " - 0s - loss: 17.9598\n",
      "episode done in 32 steps, total reward 93.0\n",
      "ep 74 / 100 noise_level 0.26\n",
      "Epoch 1/1\n",
      " - 0s - loss: 12.8771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 18.7474\n",
      "episode done in 28 steps, total reward 81.0\n",
      "ep 75 / 100 noise_level 0.25\n",
      "Epoch 1/1\n",
      " - 0s - loss: 54.6228\n",
      "Epoch 1/1\n",
      " - 0s - loss: 12.3492\n",
      "episode done in 21 steps, total reward 62.0\n",
      "ep 76 / 100 noise_level 0.24\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.8151\n",
      "Epoch 1/1\n",
      " - 0s - loss: 19.1317\n",
      "episode done in 24 steps, total reward 70.0\n",
      "ep 77 / 100 noise_level 0.23\n",
      "Epoch 1/1\n",
      " - 0s - loss: 12.8305\n",
      "Epoch 1/1\n",
      " - 0s - loss: 20.7669\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 78 / 100 noise_level 0.22\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.2854\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16.0635\n",
      "episode done in 32 steps, total reward 94.0\n",
      "ep 79 / 100 noise_level 0.21\n",
      "Epoch 1/1\n",
      " - 0s - loss: 11.3614\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.6801\n",
      "episode done in 52 steps, total reward 153.0\n",
      "ep 80 / 100 noise_level 0.2\n",
      "Epoch 1/1\n",
      " - 0s - loss: 18.2376\n",
      "Epoch 1/1\n",
      " - 0s - loss: 11.9198\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 81 / 100 noise_level 0.19\n",
      "Epoch 1/1\n",
      " - 0s - loss: 33.6089\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.9445\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 82 / 100 noise_level 0.18\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16.5275\n",
      "Epoch 1/1\n",
      " - 0s - loss: 15.1923\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 83 / 100 noise_level 0.17\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.6018\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16.3355\n",
      "episode done in 48 steps, total reward 141.0\n",
      "ep 84 / 100 noise_level 0.16\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.9926\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.9526\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 85 / 100 noise_level 0.15\n",
      "Epoch 1/1\n",
      " - 0s - loss: 10.8539\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.3989\n",
      "episode done in 35 steps, total reward 103.0\n",
      "ep 86 / 100 noise_level 0.14\n",
      "Epoch 1/1\n",
      " - 0s - loss: 12.2984\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.8651\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 87 / 100 noise_level 0.13\n",
      "Epoch 1/1\n",
      " - 0s - loss: 14.0700\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.9699\n",
      "episode done in 68 steps, total reward 201.0\n",
      "ep 88 / 100 noise_level 0.12\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.0874\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.7398\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 89 / 100 noise_level 0.11\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16.5102\n",
      "Epoch 1/1\n",
      " - 0s - loss: -1.0555e-01\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 90 / 100 noise_level 0.1\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.6712\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.8038\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 91 / 100 noise_level 0.09\n",
      "Epoch 1/1\n",
      " - 0s - loss: 130.3716\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.3855\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 92 / 100 noise_level 0.08\n",
      "Epoch 1/1\n",
      " - 0s - loss: 10.4773\n",
      "Epoch 1/1\n",
      " - 0s - loss: -1.2670e+00\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 93 / 100 noise_level 0.07\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.4482\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.5588\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 94 / 100 noise_level 0.06\n",
      "Epoch 1/1\n",
      " - 0s - loss: 15.4238\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.4638\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 95 / 100 noise_level 0.05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 17.4156\n",
      "Epoch 1/1\n",
      " - 0s - loss: -2.3098e+00\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 96 / 100 noise_level 0.04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 18.4492\n",
      "Epoch 1/1\n",
      " - 0s - loss: -1.5483e+00\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 97 / 100 noise_level 0.03\n",
      "Epoch 1/1\n",
      " - 0s - loss: 11.9209\n",
      "Epoch 1/1\n",
      " - 0s - loss: -7.7629e+00\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 98 / 100 noise_level 0.03\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.4955\n",
      "Epoch 1/1\n",
      " - 0s - loss: -3.9200e+00\n",
      "episode done in 84 steps, total reward 252.0\n",
      "ep 99 / 100 noise_level 0.03\n",
      "Epoch 1/1\n",
      " - 0s - loss: 78.8750\n",
      "Epoch 1/1\n",
      " - 0s - loss: -5.4636e+00\n",
      "episode done in 84 steps, total reward 252.0\n"
     ]
    }
   ],
   "source": [
    "from agents.nnagent import nnagent\n",
    "from task import Task\n",
    "import numpy as np\n",
    "from keras.optimizers import *\n",
    "\n",
    "target_pos = np.array([20, 10, 30])\n",
    "task= Task(target_pos = target_pos)\n",
    "agent = nnagent(task,\n",
    "discount_factor=.995,\n",
    "optimizer=RMSprop()\n",
    ")\n",
    "\n",
    "\n",
    "episode_recode= {x:[] for x in ['episode', 'score']}\n",
    "# labels = ['time', 'reward','x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',\n",
    "#           'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',\n",
    "#           'psi_velocity']\n",
    "# result = {x : [] for x in labels}\n",
    "def r(ep):\n",
    "    e = task\n",
    "    for i in range(ep):\n",
    "        noise_level = max(3e-2,(float(ep)-i)/float(ep))\n",
    "        print('ep',i,'/',ep,'noise_level',noise_level)\n",
    "        if i ==(ep-1):\n",
    "            \n",
    "            total_reward, result = agent.play(e,max_steps=1000,noise_level=noise_level, record = True)\n",
    "            episode_recode['episode'].append(i)\n",
    "            episode_recode['score'].append(total_reward)\n",
    "            \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            total_reward = agent.play(e,max_steps=1000,noise_level=noise_level)\n",
    "            episode_recode['episode'].append(i)\n",
    "            episode_recode['score'].append(total_reward)\n",
    "    \n",
    "        \n",
    "    return result\n",
    "\n",
    "result=r(100)\n",
    "\n",
    "\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plot the Rewards\n",
    "\n",
    "Once you are satisfied with your performance, plot the episode rewards, either from a single run, or averaged over multiple runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Plot the rewards.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(episode_recode['episode'], episode_recode['score'], label='score')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8m9Wd7/HPkWxJtjbvux3HTpzNCYEskEKZJt0opWwthVK2ApNyp53S6dxOO1OGttN2psx0Wtrbmelwp0BpKZSyleVC2SFAEpKQfXcWx/tuy7YsW8u5fzyKkpCEhFjyo+X3fr30ki3Jfn4K5qvznOcsSmuNEEKI9GcxuwAhhBBTQwJfCCEyhAS+EEJkCAl8IYTIEBL4QgiRISTwhRAiQ0jgCyFEhpDAF0KIDCGBL4QQGSLL7AKOVlRUpGtra80uQwghUsqGDRt6tdbFp3pdUgV+bW0t69evN7sMIYRIKUqp5tN5nXTpCCFEhpDAF0KIDCGBL4QQGSKp+vBPJBgM0traSiAQMLuUk3I4HFRVVZGdnW12KUIIcVJJH/itra243W5qa2tRSpldznG01vT19dHa2sr06dPNLkcIIU4q6bt0AoEAhYWFSRn2AEopCgsLk/oMRAghIAUCH0jasD8s2esTQghIkcAXQoi0pTW8dhd0bkv4oZK+D18IIdLamz+D1/4ZQgEoa0zooaSFL4QQZnn3AXj5+zD/Kljxjwk/nAT+Kaxbt44FCxYQCAQYHR1l3rx5bNuW+FMvIUSa2/UsPH071H8ULvtPsCQ+jlOqS+f7T29nR7svrr9zboWH735m3kmfX7JkCZdeeil33HEHY2NjXHfddTQ2Jva0SwiR5prfhkdvhoqz4fMPQJZtSg6bUoFvljvvvJMlS5bgcDj4xS9+YXY5QohU1rkNfn8NeKvh2j+C3TVlh06pwH+/lngi9fX1MTIyQjAYJBAI4HQ6TalDCJHiBg7C764EmxOufwKchVN6eOnDPw1f/vKX+cEPfsAXv/hFvvWtb5ldjhAiFY10w2+vgNC4EfZ51VNeQkq18M3wwAMPkJ2dzbXXXks4HOZDH/oQr7zyCitWrDC7NCFEqgj44HefBV8H3PgUlMw2pQwJ/FO44YYbuOGGGwCwWq2sXbvW5IqEECklOAYPXQPdO+Cah6B6qWmlSOALIUSihIPwyI3GqJzP/g80fMLUciTwhRAiESJheOLLsPfPcMnPYP7nzK5ILtoKIUTcaQ3P/i1seww+9j1YfLPZFQES+EIIEV9awwt3wIb74PyvwwV/Y3ZFMRL4QggRT6/9C6z+JSxdabTuk4gEvhBCxMubd8Prd8HC6+CiuyDJ9sqQwBdCiHhYew+89F1o/Cxc+ospWQztg0q+ioQQItVs/B08902Y9Wm44r/BYjW7ohOSwD+FO++8k7vvvjv2/Xe+8x1+/vOfm1iRECKpbHsMnvprqF8BV90H1myzKzqpSY/DV0o5gDcAe/T3Paq1/q5SajrwMFAIbACu11pPTOpgz30bOrdOsuL3KJsPn/rxSZ+++eabufLKK/n6179OJBLh4Ycf5p133olvDUKI1LT7OXh8JVSfB1c/CFl2syt6X/Fo4Y8DK7TWZwELgYuUUucBdwE/01rPAAaAW+JwrClXW1tLYWEhGzdu5IUXXuDss8+msHBqV7gTQiShfa/AIzdA2QK49g9gyzW7olOadAtfa62Bkei32dGbBlYA10Yf/w3wPeC/JnWw92mJJ9Ktt97K/fffT2dnJzffnBwTKIQQJtr/Ojz0BShqgOseA4fH7IpOS1z68JVSVqXUJqAbeBHYBwxqrUPRl7QClfE4lhmuuOIKnn/+edatW8cnP/lJs8sRQpip+W1jMbT86XDDnyC3wOyKTltc1tLRWoeBhUqpPOAJ4LTX/lRKrQRWAtTU1MSjnLiz2WwsX76cvLw8rNbkvPouhJgCLe/Ag1eBt8pY5thZZHZFH0hcR+lorQeBV4FlQJ5S6vAHShXQdpKfuUdrvVhrvbi4uDie5cRNJBJhzZo13HJLSl6GEELEQ+sGY017Vync+DS4Ssyu6AObdOArpYqjLXuUUjnAx4GdGMF/eHm4G4E/TfZYZtixYwczZszgox/9KDNnzjS7HCGEGdo2GLtV5RYaYe8uM7uiMxKPLp1y4DdKKSvGB8gjWutnlFI7gIeVUj8ENgK/jsOxptzcuXPZv3+/2WUIIczSvjEa9vlw0zPgTdnLkXEZpbMFOPsEj+8HzNvaRQghJqt9EzxwOTi8cOMzRt99CpOZtkIIcSJt78IDl4LdbYS9CZuOx5sEvhBCvFfr+mjLPg9uehbyp5ldUVxI4AshxNEOrTXCPrcgrcIeJPCFEOKI5tXwuyuNIZc3PZsW3ThHk8AXQgiAg28Z4+zd5UbYp/BonJORwD8Nv/rVr1i4cCELFy5k+vTpLF++3OyShBDxdGAVPPg5I+RvehY85WZXlBBxWVphqtz1zl3s6t8V1985u2A231r6rfd9zW233cZtt91GMBhkxYoVfOMb34hrDUIIE+1/HX5/tdFXn6IzaE+XtPA/gNtvv50VK1bwmc98xuxShBDx0PRSNOxrjaGXaRz2kGIt/FO1xBPp/vvvp7m5mV/+8pem1SCEiKNdz8Ifb4LiWXD9kym3ENqZSKnAN8uGDRv4yU9+wqpVq7Ak4cbEQogPaOujxk5VFWfDdY9CTr7ZFU0JSa/T8Mtf/pL+/n6WL1/OwoULufXWW80uSQhxpt79LTx2K9ScBzc8mTFhD9LCPy333Xef2SUIIeJhzX/B8982Nhy/+sGU2JYwniTwhRDpT2t44yfw6g9h9iXwuXuTfsPxRJDAF0KkN63hxTvh7V/Agmvgsv8Aa2ZGX0q8a601SimzyzgpYx93IUTSiYTh2W/Ahvthya3wqX+DDB54kfTv3OFw0NfXl7ShqrWmr68Ph8NhdilCiKOFJuCxW4ywv+AbcPFPMjrsIQVa+FVVVbS2ttLT02N2KSflcDioqkrtjRGESCsTfnjkemNi1cf/Cc6/3eyKkkLSB352djbTp083uwwhRKoYGzRmz7ashc/8AhbdaHZFSSPpA18IIU7bcKex4mXPbmMkTuOVZleUVCTwhRDpof8A/PZyGOmGa/8AMz5qdkVJRwJfCJH6urbDb6+A8ATc8BRULzG7oqSU2ZeshRCp7+BbcO+nQFngS89J2L8PCXwhROra+bTRsneVwC0vQMkcsytKahL4QojUtP5eeOQGKF9ghH1ejdkVJT3pwxdCpBat4dV/hjf+FWZ+Eq66D2xOs6tKCRL4QojUEZqAp78Gmx+Cs6+DS+4Ga7bZVaUMCXwhRGoIDBldOPtfg+XfgQu/CUm8xlYyksAXQiS/oTZ48Cro3Q2X/xcsvNbsilKSBL4QIrl17YAHPwcBH3zxj8bmJeKMSOALIZLXgTfg4euMnalufg7K5ptdUUqTYZlCiOS05Y/w2yvBUw63vChhHwcS+EKI5KI1vHYXPH4rVJ8LNz8PedVmV5UWpEtHCJE8QuPw1F/Dlj/AWdfCZ34OWTazq0obEvhCiOTg74eHvwiH3oYVd8CH/7cMu4wzCXwhhPl69xrDLn3t0XXsP2t2RWlJAl8IYa79rxvbEVptcNOzstplAk36oq1Sqlop9apSaodSartS6vbo4wVKqReVUnuj9/mTL1cIkVY2/AZ+dyW4K+DWlyXsEyweo3RCwN9qrecC5wFfUUrNBb4NvKy1ngm8HP1eCCEgHILnvmWsizP9L+CWP0P+NLOrSnuT7tLRWncAHdGvh5VSO4FK4DLgI9GX/QZ4DfjWZI8nhEhx/n549EvGmjjLvgof+z5YpXd5KsT1X1kpVQucDawFSqMfBgCdQGk8jyWESEHdu+Cha8DXBpf9h7HipZgycQt8pZQLeAz4utbap44aTqW11kopfZKfWwmsBKipkQ0MhEhbu5+Hx26F7By48RmoOdfsijJOXGbaKqWyMcL+Qa3149GHu5RS5dHny4HuE/2s1voerfVirfXi4uLieJQjhEgmWsObPzNa9oV1sPJVCXuTxGOUjgJ+DezUWv/0qKeeAm6Mfn0j8KfJHksIkWKCAXjiy/DS92De5fCl58FbZXZVGSseXTrnA9cDW5VSm6KP/QPwY+ARpdQtQDPw+TgcSwiRKgZbjPH17Rth+R1wocycNVs8Rum8CZzsv+JHJ/v7hRAp6MAb8MebIByEax6C2RebXZFAZtoKIeJJa1jzn/DCP0JhPVzzeyiaaXZVIkoCXwgRH+MjxkqX2x+H2ZcYWxE6PGZXJY4igS+EmLzeJvjDdcaesx/9Lpz/dbDIdhvJRgJfCDE5O5+BJ24z1q2/7nGoX252ReIkJPCFEGcmHIKXvw9v/wIqzoHPPyA7UyU5CXwhxAc33AmP3gzNb8HiW+Cif4Esu9lViVOQwBdCfDAH3zTCPuCDK+6Bs642uyJxmiTwhRCnJxKBN38Kr/4ICurg+iegdJ7ZVYkPQAJfCHFqo33wxEpoegkaPwefuRvsbrOrEh+QBL4Q4v01r4bHboHRXrjkZ7DoS7JEQoqSwBdCnFgkAm/9DF75kbEb1S0vQMVCs6sSkyCBL4Q43kgPPP6XsP9VaPwsXHK3zJpNAxL4Qohj7X0JnvxfMO4zgn7RTdKFkyYk8IUQhmDAmEi15j+heA7c8KSMwkkzEvhCCGOv2cduha6tsHQlfPyfjK0IRVqRwBcik2kNG+6D5/8BbLlw7SPQ8EmzqxIJIoEvRKby98PTX4OdT0PdcrjiV+AuM7sqkUAS+EJkon2vwp++AiPd8PEfwLKvynLGGUACX4hMMuE3NhR/57+hqAFufREqzja7KjFFJPCFyBRt78ITX4bePXDubfCx78mF2QwjgS9EugsHYdW/w+v/avTRX/+kbFKSJCbCE6zvWs+q1lUsKVvCipoVCT2eBL4Q6axnt9Gqb98IC66BT90FOXlmV5XResd6eaP1DV5reY01HWsYC41hs9gocBRI4AshzkAkbEygeuWHkJ1r7EY19zKzq8pIWmuaBpt4vfV1Xmt5jS09W9Boyp3lXFp/KR+u/DBLy5eSk5X47jUJfCHSTc9uYwRO6zqY9WljhUt3qdlVZRR/0M/ajrWsalvFqrZVdI52AjC3cC5/tfCvWF69nIb8BtQUL1khgS9EugiHYPX/gVf/BWxO+OyvjYXPZB2cKdEy3MKq1lW80fYG6zrWMRGZIDcrl2UVy7htwW1cUHkBpU5zP3gl8IVIB1074E9/ZfTVz/kMfPqn4Coxu6q0NhYaY13nOt5se5O329+m2dcMwDTPNK6efTUXVl3IopJFZFuzTa70CAl8IVJZOAhv3g2v32UsX3zV/TDvCrOrSlutw62salvF662vx1rxDquDJWVL+MLsL3B+xfnUemvNLvOkJPCFSFVt78JTXzMWPJt3JVz8b+AsMruqtOIP+lnXuY632t86phVf467h87M+z4crP8yiskXYrXaTKz09EvhCpJqJUXj1n41ROM4SuPp3RjeOmLRwJMyu/l283f42b7e/zaaeTYQiIXKyclhcupgvzP4CF1RewDTPNLNLPSMS+EKkkr0vwrPfgMFDsPhmY7asw2t2VSmtc7ST1e2rebv9bdZ0rGFwfBCAOQVzuGHuDSyrWMY5Jedgs9pMrnTyJPCFSAW+Dnj+27DjSWMNnC89B9M+ZHZVKWl4Yph1netY3b6aNR1rOOg7CEBxTjEXVl3IsoplLCtfRmFOobmFJoAEvhDJLBKG9ffCy/8EoXFYfgec/zXISo0+42QwEZ5ga+9WVrevZnXHarb1biOiI+Rk5bCodBGfa/gcyyqWMTNv5pSPi59qEvhCJKv2jfDM3xj3dcvh0/8OhfVmV5X0guEgm3s2s65rHRs6N7CpZxPj4XEsykJjUSO3zr+V88rPY2HxwqQaMjkVJPCFSDaBIXjlR7Du/4KzWCZQnUI4Emb3wG7e6XiHNZ1reLfrXcZCYygUswpmcVXDVSwpW8LissV4bB6zyzWVBL4QySISgS1/gBfvhNEeWPqXsOIOuSj7HsFIkB19O1jfuZ71XevZ1L2JkeAIANO907l8xuWcW34ui0sX47XLv93RJPCFSAYdm+H/fRNa1kLlYrj2D1B5jtlVJYVAKMDW3q1s7N7I+s71bOrZxFhoDIA6bx0XT7+YRaWLWFS6yPSlC5JdXAJfKXUvcAnQrbVujD5WAPwBqAUOAp/XWg/E43hCpA1/P7zyA9hwP+QUwGX/AWddm9HbDQ6ND/Fu17u8223cdvTtIBQJATAzfyaXz7icxaWLWVS6KC1H0iRSvFr49wO/BB446rFvAy9rrX+slPp29Ptvxel4QqS2cAg23GcsXzw+DEtXwkf+PuPWqtda0zLcwqaeTWzs3sim7k00DTYBkG3JZn7RfG6ceyPnlJ7DWcVnSRfNJMUl8LXWbyilat/z8GXAR6Jf/wZ4jQQF/vbe7fxxzx8JRoKEdZhQJIRC4chy4LA6yMnKwWP3kG/PJ9+RT2FOIZWuSgodhWk/DEskoX2vwgt3QNc2mH4hfOpfoWSO2VVNiZGJEbb2bmVr71a29Gxha+9W+gP9ALiz3SwoXsDF0y/mnNJzaCxqTJklC1JFIvvwS7XWHdGvO4ETdq4ppVYCKwFqamrO6EA9Yz280foGWZYssixZWJUVgEA4QCAUYCw0xnh4/Lifc1gdVLoqme6dTmNRI41FjcwrnIfL5jqjOoR4X9274MV/hL0vQF4NXPUbY1OSNG10hCNhDgwdYGvvVjb3bGZzz2b2De5DowHjAusFlRewsGQhC4sXUp9Xj0VlblfWVFBa6/j8IqOF/8xRffiDWuu8o54f0Frnv9/vWLx4sV6/fn1c6nmvQCjAQGCA/vF++sb6aB1upW2kjZbhFpoGm2gZbjHqRFGfVx/7Izyr+CymeabJmYA4c6O9xto3G+4Hmwsu/FtY+mXIdphdWdxorekY7WBL7xa29Wxja+9WdvbvjF1cddvcLChawFnFZ7GgeAHzi+dn/BDJeFJKbdBaLz7V6xLZwu9SSpVrrTuUUuVAdwKPdUqOLAflrnLKXeUnfH4wMMj2vu1s6d3Clp4t/Pngn3l0z6MAeO1eFhQtYEHxAhYULaCxuFH+WMWpBQOw9lfGBuITo8baNx/5e3Cm9oXGw/3uu/p3xW7b+7bHumZsFhuzC2Zz+YzLjTPnwkZqvbXSek8CiQz8p4AbgR9H7/+UwGNNWp4jj/Mrz+f8yvMBiOgIB4YOsKl7U+xD4M22N2Ono7WeWhYUL2Bu4VzmFc5jVsGsKdmTUqSASAS2P24shzDYDDM/CZ/4IRQ3mF3ZBxaOhDk0fIjdA7vZ0bcjdhueGAbAqqzU5dVxQeUFscZQQ15Dxs1gTRVx6dJRSj2EcYG2COgCvgs8CTwC1ADNGMMy+9/v9ySySycehieG2dZrnK5u7dnKlt4tsVaNRVmo89Yxt3CufAhkKq2h6WV4+XvQuRVKG+ETP4D6FWZXdlp8Ez729O9h98Bu9gzsYU//HpoGmwiEAwBkWbJoyG9gXuE85hbOZU7hHGbkzZALq0ngdLt04taHHw/JHvjvpbWm299ttHr6jZbP9t7t9AX6AON6wDTPNGbmz6Qhv4HZBbOZlT+LMmeZXBNINy3vwEvfh+Y3IW8arPhHYzmEJBxP7w/6OTB0gKbBJvYN7mPv4F6aBptiG20D5NvzaShoYFb+LBryG2jIb6A+rz4tlghORxL4Jjn8IbCtbxu7+42W0t6BvbQMt8S6gzw2D7MKZlHvrac+z7jVeetkEkkq6tphTJza/f+MdW8u/DtYdBNkmR+MIxMjNPua2Te0j32D+9g/uJ+mwSbaRtpif4s2i426vDpm5M2gPq+eWfmzmFUwi+KcYmmUpBAJ/CTjD/rZM7CH3f272TWwiz39e9g/tD+2BggYraoZ+TOo99Yz3TudWm8ttZ5aypxlcsEr2fTtM/aR3fII2N3GksXn/i+wT+2Q3mAkSNtwGwd9BzkwdICDvoMcHDpIs685dqYJRndMrac21sCYmTeT+rx6qt3VZFlkhZVUJ4GfAg6fDewb3Bdrhe0d3Mu+wX2MBkdjr7Nb7VS7q6n11DLNM41pnmlUu6updldTnFssHwZTqf8AvPET2PwQWG3GAmcX/A3kFiTskP6gn9aRVlp8LRwaPsSh4UO0DLfQOtxK52gnYR2OvbbAUUCtp5Zab/RvxT2Nurw6CfY0J4GfwrTW9I71Gq21aIvtkO8QzcPNtAy3xNYVAWPyWIWrgkpXJRWuCqpcVcb37koqnZV47V45NY+HgYPG8MpNvwdLFiy+Bc6/HdyTW6xLa83Q+BCd/k7aRtpoH2mP3TpGO+gY7YhtuXdYvj2fanc1Ve4qqtxVVLurjTNCT60sPZChkmEcvjhDSimKc4spzi1mSdmSY54LRUJ0jHbQ4muhZdho8R0Oik09m2LD5Q7LycqhNLeUktwSSnJLKM0tpcxZFrsVOgrJd+RL6+9kjg56ZTXG0l/wDfCceD7H0cZCY/T6e+ke66ZnrIcev3Hr8nfR7e+m299Nl7/ruFngOVk5VLoqKXeWM79oPuWucqpcVdR4aqh2V+O2uRP0ZkW6kxZ+mvFN+GgfaT+mtXh0uPT4ewjp0HE/l2fPoyiniMKcQopyiihyFJHvyCfPnkeePQ+v3Wt87TC+zrak+Tjr3iZ486ew+WGwZBE+50Z8S29m0ObAN+HDN+7DN+FjcHyQ/oAxe7s/0E9foI/+sX76A/34Q/7jfq3NYjvmw7fUWRr7+vBZWp49T87KxAciXTrihMKRMH2BPrpGu+jydx0TVL1jvcfcTrT+0GGubBcumwu3zY07243L5sKZ5cRpcxr32U5ys3NxZjvJycrBYXVgs9pwZDmwW+3YrfbY14fXP7IoC9mW7NiaSKdzbUJrTURHYovmHX0/EZ4gGAkyEZ5gIjzBWGiMsdBYbI2l2H0owEhwhJGJEYZ9rYz2bGd0pAu/1cpIjpchi4Xh4GhsZMt7WZSFfHs+BTkFFDoKKcwppMBRQIGjgOKcYopziinKLaIkp0S62ERCSJeOOCGrxRprYc5n/klfp7VmLDTG0PgQg+ODDI4Pxr4eGB/AN+5jeGLYuAWH6R3rpTnYzMjECKPB0dhknUnVqqxGOJ4oZxWgOeHZypnIUlbcGlzBAC6tcHqrKCmYQW1OPh6bJ3a247V78dg8xs3uiZ0ByYVzkQok8MUJKaXIzc4lNzv3pOsPvZ9QJIQ/5Mcf9OMP+ZkITzAeHmc8NE4gHGA8PE4gZNyHI2HCOhxrnR++BSPBWKtacaRVfHRL+/DZgVVZsVqsZKms2L3NaiPbmo3NYsNmtRlnGkctmW23ZOM48AaOtfdga12Pyi2C824zLsgmcNSNEGaRwBcJkWXJirWEk04wAFsehrf/D/Q1QX4tfPrfYeEXIVuWwhDpSwJfZI6Rblh/L6z7NYx2Q/lCuOp+mHMpWKxmVydEwkngi/TXsRnW/Aq2PQrhCWP1ymVfMXabkguoIoNI4Iv0FAnDrmeN9eib34JsJ5xzI5x7GxTNMLs6IUwhgS/Sy0g3bPwdrL8Phg4ZWwl+4odw9vUZt0G4EO8lgS9Sn9Zw4A3YcB/sfAYiQaj9MFz0zzDrYumfFyJKAl+kLn+/sYjZ+nuN0TY5+bB0pbE8cQruLiVEokngi9QSicDBVbDpQdj+JITHofpcuPCbMPcyGVYpxPuQwBepYeAgbHoINv8eBg+B3QtnX2csZlbWaHZ1QqQECXyRvMZHYOdTxkqVB1cBCur+AlbcCXMukda8EB+QBL5ILuEQ7H/V2Elq17MQHIX86bD8DjjramPUjRDijEjgC/NFItCyFrY/DtufgNEecOTBgqtgwTVQc55MkBIiDiTwhTm0hrZ3j4S8rw2yHNDwSVhwNcz4GGTZza5SiLQigS+mTiQC7RthxxOw/U/GxChLthHuH/sezPqUsSG4ECIhJPBFYoXG4cAq2P0s7H4OhjuMkK9fDh/5Nsy+2Bg/L4RIOAl8EX+BIdj7Iux6Bva+BBPDxlo2Mz4Ksz9tdNtIyAsx5STwxeRpDV3boOklaHoZDq2GSAicxdB4Bcz6NNR9BLIdZlcqREaTwBdnxt9vDJ9setkI+pEu4/Gy+bDsq8YaNlWLZR0bIZKIBL44PROjRsv9wCpjElT7RtARo2umbjnM/DjUrwB3mdmVCiFOQgJfnNiEH1rfORLwbRuMbhpLttFyv/CbMOPjUHmOtOKFSBES+MIw3GVMfmpZC4fWGLtERYKgrFBxNnzor40lh2vOA5vT7GqFEGdAAj8TBQPQudVotbe+A63rjAXJAKx2o9W+7CtQe4ER8DI2Xoi0IIGf7ib80L0TOjdDxxaj771rm9E9A+CugOolxjryVUuhYqHMcBUiTUngpwutjeUJundB11ajBd+5Dfr2GhdXARxeKF9odM9UnGO05L1V5tYthJgyEvipJhw01obv2QU9u6F3T/R+r7Gy5GHeGmOd+HmXQ9kCY7hkXo0sQiZEBpPATzaRCPj7YLDZCPaBA9H7ZuPmaz3SYgfwVELxLDjneihqMG5ljTKTVQhxnIQHvlLqIuDngBX4H631jxN9zKQUDsFYv7H072gPjPQY68oMd0bvO4wumeFOCE8c+7OuUsivNS6g5tdCQZ2xZ2tRg1xQFUKctoQGvlLKCvwH8HGgFVinlHpKa70jkcc9Y+EQBP0QCkRv48Z9JGS0vCMhI4yDfmMiUnDM+DroN76eGDXWkQkMQWAQxgZhbMC4Hx868TGzc8FdbtyqzwNPhXHLm2aEe14N2HKn9J9BCJGeEt3CXwo0aa33AyilHgYuA+Ie+FprQqEQwfFRQmN+Qv5B9GgPerQHRnuxjPVjmfBhGR/CMj6IJTCEJTCIZXwI64QPSyiAikyc+kDvJ8thXBh15Bn3zmKjuyUn33jMWWQ8dvjmLjNa6NKvLoSYAokO/Eqg5ajvW4Fz432QDc/dT+Oa/41dBcl+n9cFdDZDOBnSzqPuaxnWuYxhx6/t+LEzgQ1HjhOXy4nX5aSiwE1VkZuaQjeu3Fxj5UdbrrGnarYzep8jM06FEEnN9Iu2SqmauJW9AAAPjUlEQVSVwEqAmpoz26+0cNpcNrd+AZ2VQyQrx7i3uQg6igg6Cgk5Cgg78olYj4wvt1oUVqVwWRRODRPhCOPBMIFQhB5fgNaBMTYPjHGwa5TupvHoT4Up9UxQU5BFdYGVmgIrpR4LBc4wRa4Jilx2KvNyyLJaJvvPIoQQcZfowG8Dqo/6vir6WIzW+h7gHoDFixfrMzlI7dyl1M5deqY1nlLvyDg72n1sb/exr2eEQ/1+Vu/r44mNbej3VJxtVUwrdFJf7GRmiZvZ5W7mlHuoLXRitUjXjRDCPIkO/HXATKXUdIygvwa4NsHHjLsil50LG4q5sKH4mMfHQ2H6RyfoG5mgb3SCLl+AA72j7OseYW/3CC/t7CYcMT4RcrKtVObnUOZxUOKxU+pxUOZxUOqxU+JxUJ2fS7FbZrgKIRInoYGvtQ4ppb4K/BljWOa9WuvtiTzmVLJnWSn35lDuzTnh84FgmKbuEXZ2+NjVOUz74BidvgBr94/S5QsQihx7elDksjGn3MOccg8zS1zMiN7cjve7MiGEEKdH6ff2SZho8eLFev369WaXMSUiEU2/f4LOoQBdvgAH+/zs7PCxs8PH3q4RJsJHJlcVu+2UuO0UuewUumyUeRzUFORSU5hLTUEuFd4cLNJdJETGUkpt0FovPtXrTL9om6ksFkWRywjxxkrvMc+FwhEO9ftp6h6hqWeEg72j9I5M0DsyTlP3yHFnBy57FnPLPcyr9DC33MO0QifVBTmUuh3yQSCEiJHAT0JZVgt1xS7qil184gTPh8IROoYCHOr3c7BvlF0dw2xvH+Khdw4RCB45M7BZLZR5HRS77RS5bBS77dQWOplZ6mZWqZtSjx0lcwCEyBgS+Ckoy2qhuiCX6oJczp9RFHs8HNE0943SMjBGS7+flgE/HYMBekfGOdA7ypr9/QyNBWOv9ziymF3mYU50JFF9iYtSt3FR2ZEtcwqESDcS+GnEalGxM4OT6R+dYE/XMHu6htndOcyuzmEe3dDK6ET4mNfl5WZT5nFQkZdDRZ6DyrxcZpe7aazwymgiIVKUBH6GKXDaOK+ukPPqCmOPRSKalgE/zX1+unzGReROX4DOoQDtgwHePTTAoP/ImUGJ287scg8zil3UFTupL3ZRkeegyGXHaZc/KSGSlfzfKbBYjMli0wpPvlft0FiQnR0+trUNsaPdx+6uYdYd6GcseOyZQa7NSonbzrRCJ9OLjAloM0rczK3w4M2R4aVCmEkCX5wWb072Cc8MOn0B9vWM0OUbp2d4nN6RcTp9AQ72jrLuYD/+o7qKphXm0ljhpa7YSVV+DpV5ubHJaDk2uWYgRKJJ4IszZrGoaB//iSeeaa3p8o2zp2uYrW1DbG8fYmvbEM9t6+A9c85wO7IocdupLsiNXUieXeahrthJtqxNJERcSOCLhFFKUeZ1UOZ1HLMsRTAcoXPIWKCubXCM7uEA3b5xOocCNPf7eatpP8Gw8Ylgs1qYUeJidrmbhlI35V4HpR5HbGkKOTMQ4vRJ4Ispl33UsNITCYYj7O8ZNWYed/rY1THMW029PP5u23GvLXLZqMo3fldDiYvGKi/zK70UuWQkkRDvJYEvkk621cKsMjezytxcTmXscV8gSLcvQJdvnC5fgPbBMVoHxmgZ8LOpZYCnN7fHXlvmcTCjxMX0Iid1xU6q83Mp8dijk9Ds0k0kMpIEvkgZHkc2Hkc2M0pOvI/vcCDI9vYjI4n29Y7y5KY2hgOh415bmZfDjBIXM0tczCx1Ma/CS0OpG1uWfBCI9CWBL9KG23H8SCKtNX2jE7QOjNEzbIwkMharG2Vv1whr9vcxHjKWo8i2KmaVGdcKqqIXoyvzjftyr4Ncm/zvIlKb/AWLtKbUkUXqTiQc0Rzq97O9fYhtbcbZwep9fXT5AseNJPI4sqjIy2FOubFI3bwKYynrfKdtCt6JEJMnyyMLcQKHRxK1DY7RORSgYyhA59AYh/r97OwYptMXiL22xG03rjmUuqkpzKXEbWxsU+Z1yIqlYkrI8shCTMKpRhL1jYyzvd3Hrk5jc5vdncM8sKaZiVDkmNfZsizUFOQyrSCXGSUuGiu9LKjyUlOQKyuViikngS/EGSg8wbaX4YimP7rVZfewcVZwqM9Yo+hg3yirmnpjHwgeRxYzS93UFjqZXpRLTaGTcq+DErex/aWsVioSQQJfiDixWhTFbnt0NVHvcc9PhCLs6RpmW9sQW9qG2Nc9wptNPTz27vhxry1y2WgoNS4gzyozlq+eXeaWDwIxKRL4QkwRW5aFxkovjZVerjnqcf9EiEP9frqj8wu6h8dp7htld9cIj6xvia1HZLUoZpa4mFvuoSo/h/LDI4nyjGWsZRSROBX5CxHCZLk2YyOa2WXHPxeJaFoHxtjRMRSbY7B6/4lHEeXnZlORl0NDqdv4YKnwMK/Si0uWrBZR8pcgRBKzWJSxWX1hLhc1lsceD4YjdPmM6wRt0TWJ2qIzj9/e18sTG48sQ1GZl0NDqYuGMjfTC52URkcPlXrsFDhtcvE4g0jgC5GCsq0WqvJzqcrPZUnt8c93DwfY3u4z9i7oNHY4e6upj4nwsaOInDYr04udTC9yUV/spLHCGEVU4nFMzRsRU0oCX4g0VOJ2UDLLwfJZJbHHQuEIHUPGNYLu6K5mzX1+DvSOsqllgGe2tHN4Wk6px86ccg/TCowRRNMKcmMzjvNys+WsIEVJ4AuRIbJOMbfAPxFiR7uPLa3GvgV7uobZcHCA4fFj1yKyZVmo8DqYWWqMHpoTHUVUU5Ark8ySnAS+EAIwLh4vri1gcW1B7DGtjbkFzf1+OoeMfY67fMZeBrs6fby8syt28dhlz2JOuZu55R5qi5yUe40zgoq8HIpccq0gGUjgCyFOSilFoctO4UnWIhqbCLOna5hdnb7YNYNHN7QyOnHsXseO7MPXHHKoK3LRWOlhQZWX6UUurHJWMGUk8IUQZyzHZuWs6jzOqs6LPXb4rKAjugaRsW+Bn5Z+Y++Ctfv7GQsaHwhOm5WZpW5mlLiYUeKivtgV29Ws0GmTLqI4k8AXQsTV0WcFjZXHzzgOhSPs6xlla9sQ26LXCl7f08OjG1qPeV1WdM/k+mIn9cXGB8Kccg+zy93Ys2TG8ZmQwBdCTKmso3Y0+9yiqtjjQ/4gB/pGY9cJunwBDvX72dczytv7jt23oKHUTWOFl9oiJ9UFOdQUGENU82UE0fuSwBdCJAVvbjYLc/Og+vjnDs843t5urEO0rW2Il3Z20Tc6cczr7FkWyr0OyrzGFpdzy73MrfAwq9QtG94j6+ELIVLYyHiIln4/Lf1+WgfG6PQduW6wp3M4NqTUoqC2yBnbvGZGiSv2wVDktKf8tQJZD18IkfaMoaDGzmPvpfXhswIfOzp87OzwsbllkGe3dBzzumyrca2gpiCXaYW51BY6mVvhobHSi8eRPVVvZUpI4Ash0pJSKjbR7KLGIyvT+QJBmnv9dPqMXczahwK09Ps51O/n6c0dDI0FY6+tK3Iyp8JDfbGx9ERdkYuq/JyUnW0sgS+EyCgeRzbzq7zMP8GeBQC9I+Nsi14n2NJq3D+3teOY1UltWRZKPXbKvTnUF7toKHUxs8TN7HL3SfdPTgbShy+EEKcwHgrT3OdnX/cI7UOB2FpEbQNjNPWMMOg/clZQ5nHQWGl0CdUXG2cElfk5FLvsCTsrkD58IYSIE3uWNbYD2XtprekZGWdv1wg7O4w9C7a1+3h5VzdHt6ftWRYq8nKoyHNQmZfDtEInc8s9zKvwTNnqpJMKfKXUVcD3gDnAUq31+qOe+3vgFiAMfE1r/efJHEsIIZKRUspYndTt4PwZRbHH/RMhWgeMWcbGvbFnQfvgGK/v6aHLd2SiWZHLzpcvrOMvL6xLaK2TbeFvA64E/vvoB5VSc4FrgHlABfCSUqpBax0+/lcIIUT6ybVlnfSsAIyLxzvbjTWItrf7KPEkvu9/UoGvtd4JnKhf6jLgYa31OHBAKdUELAVWT+Z4QgiRLjyObM6tK+TcusIpO6YlQb+3Emg56vvW6GPHUUqtVEqtV0qt7+npSVA5QgghTtnCV0q9BJxge2W+o7X+02QL0FrfA9wDxiidyf4+IYQQJ3bKwNdaf+wMfm8bx66IURV9TAghhEkS1aXzFHCNUsqulJoOzATeSdCxhBBCnIZJBb5S6gqlVCuwDHhWKfVnAK31duARYAfwPPAVGaEjhBDmmuwonSeAJ07y3I+AH03m9wshhIifRHXpCCGESDIS+EIIkSGSavE0pVQP0PwBfqQI6E1QOclK3nNmkPecGeL1nqdprYtP9aKkCvwPSim1/nRWiEsn8p4zg7znzDDV71m6dIQQIkNI4AshRIZI9cC/x+wCTCDvOTPIe84MU/qeU7oPXwghxOlL9Ra+EEKI05SSga+UukgptVsp1aSU+rbZ9UwFpdS9SqlupdQ2s2uZKkqpaqXUq0qpHUqp7Uqp282uKdGUUg6l1DtKqc3R9/x9s2uaKkopq1Jqo1LqGbNrmQpKqYNKqa1KqU1KqSnZzDvlunSUUlZgD/BxjHX21wFf0FrvMLWwBFNKXQiMAA9orRvNrmcqKKXKgXKt9btKKTewAbg8nf9bK2M3IafWekQplQ28CdyutV5jcmkJp5T6BrAY8GitLzG7nkRTSh0EFmutp2zuQSq28JcCTVrr/VrrCeBhjB220prW+g2g3+w6ppLWukNr/W7062FgJyfZSCddaMNI9Nvs6C21WmVnQClVBXwa+B+za0lnqRj4p72blkgfSqla4GxgrbmVJF60a2MT0A28qLVO+/cM3A38HRAxu5AppIEXlFIblFIrp+KAqRj4IsMopVzAY8DXtdY+s+tJNK11WGu9EGPjoKVKqbTuwlNKXQJ0a603mF3LFLtAa30O8CngK9Fu24RKxcCX3bQySLQf+zHgQa3142bXM5W01oPAq8BFZteSYOcDl0b7tB8GViilfmduSYmntW6L3ndjLDO/NNHHTMXAXwfMVEpNV0rZgGswdtgSaSZ6AfPXwE6t9U/NrmcqKKWKlVJ50a9zMAYn7DK3qsTSWv+91rpKa12L8f/zK1rr60wuK6GUUs7oQASUUk7gE0DCR+ClXOBrrUPAV4E/Y1zEeyS6w1ZaU0o9BKwGZimlWpVSt5hd0xQ4H7geo8W3KXq72OyiEqwceFUptQWjcfOi1jojhilmmFLgTaXUZoztX5/VWj+f6IOm3LBMIYQQZyblWvhCCCHOjAS+EEJkCAl8IYTIEBL4QgiRISTwhRAiQ0jgCyFEhpDAF0KIDCGBL4QQGeL/A65/3weJtyf4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(result['time'], result['x'], label='x')\n",
    "plt.plot(result['time'], result['y'], label='y')\n",
    "plt.plot(result['time'], result['z'], label='z')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the rewards in the latest episode\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.plot(result['time'],result['reward'], label='reward')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10.  10.  40.]\n",
      "0.0184064387483\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "target_pos = np.array([1., 1., 2.])\n",
    "self= Task(target_pos = target_pos)\n",
    "print((np.abs(self.sim.pose[:3] - self.target_pos)/(0.1*self.target_pos)))\n",
    "\n",
    "target_pos = self.target_pos[:3]\n",
    "current_pos = self.sim.pose[:3]\n",
    "reward = np.exp(-(np.abs(target_pos  - current_pos)/(0.1*target_pos))).sum()+ np.exp(-(current_pos[2]-target_pos[2])/target_pos[2])\n",
    "print(reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reflections\n",
    "\n",
    "**Question 1**: Describe the task that you specified in `task.py`.  How did you design the reward function?\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "I design my drone to move to the target I assigned, using the reward function which gets the high rewards if it moves closer to the target. Also, I give a bonus to encourage drone fly high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: Discuss your agent briefly, using the following questions as a guide:\n",
    "\n",
    "- What learning algorithm(s) did you try? What worked best for you?\n",
    "- What was your final choice of hyperparameters (such as $\\alpha$, $\\gamma$, $\\epsilon$, etc.)?\n",
    "- What neural network architecture did you use (if any)? Specify layers, sizes, activation functions, etc.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "- I use DDPG because it can learn competitive policies for all of\n",
    "our tasks using low-dimensional observations (e.g. cartesian coordinates or joint angles) using the same hyper-parameters and network structure. \n",
    "A key feature of the approach is its simplicity: it requires only a straightforward actor-critic architecture and learning algorithm with very few “moving parts”, making it easy to implement and scale to more difficult problems and larger networks.\n",
    "\n",
    "(Continuous control with deep reinforcement learning,  https://arxiv.org/pdf/1509.02971.pdf)\n",
    "\n",
    "- $\\gamma$ = 0.995, $\\tau$ = 1\n",
    "\n",
    "- For Actor I use one input layer, 3 hidden layers with tanh activation, and one output function with activation tanh. For Critic, I train 2 NN for state and action and then combine 2 NN together. Each NN are 1 input layer, 2 hidden layers with tanh activation and use the tanh activation for the final combined layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Using the episode rewards plot, discuss how the agent learned over time.\n",
    "\n",
    "- Was it an easy task to learn or hard?\n",
    "- Was there a gradual learning curve, or an aha moment?\n",
    "- How good was the final performance of the agent? (e.g. mean rewards over the last 10 episodes)\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "It seem an easy task to learn, because the score is not able to imporve within 100 episode.\n",
    "\n",
    "There is a gradual learning curve show in the above plot, whcih indicates it is easy to reach high scoure after 75 episode.\n",
    "The average rewards over the last 10 episodes is 252. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Briefly summarize your experience working on this project. You can use the following prompts for ideas.\n",
    "\n",
    "- What was the hardest part of the project? (e.g. getting started, plotting, specifying the task, etc.)\n",
    "- Did you find anything interesting in how the quadcopter or your agent behaved?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "It is tough!! It takes me two weeks staying in the library to know the concept of RL, and finally, I still cannot design a proper reward function for this project. I tried two module to build DDPG; one is from Udacity, and other is from online, https://gist.github.com/ctmakro/2e0017f25b06177af539dd410b946428\n",
    "\n",
    "I try to apply the code suggested by Udacity RL project to build Pendulum-v0, but the agent would not like to learn. So I start seeking many resources from online. If there is a good example of this project, especially for reward function, please let me know what it is. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
